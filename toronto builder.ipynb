{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d5b8a-55bb-49e0-8698-855000ba99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()  # You can replace Chrome with Firefox or any other browser\n",
    "\n",
    "# Open the URL\n",
    "url = \"https://obd.hcraontario.ca/buildersearchresults?builderLocation=toronto&page=1\"\n",
    "driver.get(url)\n",
    "\n",
    "# Apply the filter (Adjust the selector based on the actual filter element)\n",
    "filter_button = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.ID, \"filterBtn\"))\n",
    ")\n",
    "filter_button.click()\n",
    "\n",
    "# Wait for the filter modal to appear\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.CLASS_NAME, \"Modal_modal__3ulyu\"))\n",
    ")\n",
    "\n",
    "# Select the filters\n",
    "licensed_radio_button = driver.find_element(By.ID, \"licensedBool_yes\")\n",
    "licensed_radio_button.click()\n",
    "\n",
    "city_checkbox = driver.find_element(By.ID, \"city_toronto_0\")\n",
    "city_checkbox.click()\n",
    "\n",
    "# Wait for 5 seconds before closing the modal\n",
    "time.sleep(5)\n",
    "\n",
    "# Close the modal\n",
    "close_modal_button = driver.find_element(By.CLASS_NAME, \"Modal_closeBtn__BirQ7\")\n",
    "close_modal_button.click()\n",
    "\n",
    "# Function to scrape links from the current page\n",
    "def scrape_links_from_page():\n",
    "    links = []\n",
    "    rows = driver.find_elements(By.CSS_SELECTOR, \"tbody tr\")\n",
    "    for row in rows:\n",
    "        link_element = row.find_element(By.CSS_SELECTOR, \"a.title\")\n",
    "        link = link_element.get_attribute(\"href\")\n",
    "        links.append(link)\n",
    "    return links\n",
    "\n",
    "# Initialize an empty list to store all links\n",
    "all_links = []\n",
    "\n",
    "# Loop through all pages and scrape links\n",
    "while True:\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"tbody tr\"))\n",
    "    )\n",
    "    links = scrape_links_from_page()\n",
    "    all_links.extend(links)\n",
    "    print(f\"Scraped links from current page\")\n",
    "\n",
    "    # Try to find the \"Next\" button\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, \"//button/span[text()='Next']/parent::button\")\n",
    "        next_button.click()\n",
    "        # Wait for the next page to load\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(\"No more pages to scrape.\")\n",
    "        break\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Save the links to a CSV file\n",
    "df_links = pd.DataFrame(all_links, columns=['Profile Link'])\n",
    "csv_file_path = \"Builder_Profile_Links.csv\"\n",
    "df_links.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8166e6-79df-4877-a513-aa240728410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Load the CSV file containing the profile links\n",
    "csv_file_path = \"Builder_Profile_Links.csv\"\n",
    "profile_links_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()  # You can replace Chrome with Firefox or any other browser\n",
    "\n",
    "# Function to scrape content from a profile page\n",
    "def scrape_profile_content(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"body\"))\n",
    "    )\n",
    "    time.sleep(5)  # Ensure the page loads completely\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    data = {\n",
    "        \"URL\": url,\n",
    "        \"Vendor/Builder Legal Name\": \"N/A\",\n",
    "        \"Doing Business As Name\": \"N/A\",\n",
    "        \"Address\": \"N/A\",\n",
    "        \"Website\": \"N/A\",\n",
    "        \"Email\": \"N/A\",\n",
    "        \"Phone Number\": \"N/A\",\n",
    "        \"Fax Number\": \"N/A\",\n",
    "    }\n",
    "\n",
    "    # Extract Vendor/Builder Legal Name and Doing Business As Name\n",
    "    name_container = soup.find(\"div\", class_=\"ProfileHeader_nameContainer__CQ7w-\")\n",
    "    if name_container:\n",
    "        legal_name = name_container.find(\"h1\", class_=\"ProfileHeader_name__22ueK\")\n",
    "        dba_name = name_container.find(\"h2\", class_=\"ProfileHeader_name__22ueK\")\n",
    "        if legal_name:\n",
    "            data[\"Vendor/Builder Legal Name\"] = legal_name.get_text(strip=True)\n",
    "        if dba_name:\n",
    "            data[\"Doing Business As Name\"] = dba_name.get_text(strip=True)\n",
    "\n",
    "    overview_section = soup.find(\"div\", {\"id\": \"overview-tab\"})\n",
    "    if overview_section:\n",
    "        fields = overview_section.find_all(\"div\", style=\"box-sizing: border-box; min-height: 1px; position: relative; padding-left: 15px; padding-right: 15px; width: 50%; flex: 0 0 50%; max-width: 50%; margin-left: 0%; right: auto; left: auto;\")\n",
    "        for field in fields:\n",
    "            key_element = field.find(\"span\", class_=\"bold\")\n",
    "            value_element = field.find(\"p\")\n",
    "            if key_element and value_element:\n",
    "                key = key_element.get_text(strip=True)\n",
    "                value = value_element.get_text(strip=True)\n",
    "                # Normalize keys to match expected output\n",
    "                if key == \"Phone Number\":\n",
    "                    key = \"Phone Number\"\n",
    "                elif key == \"Fax Number\":\n",
    "                    key = \"Fax Number\"\n",
    "                elif key == \"Email\":\n",
    "                    key = \"Email\"\n",
    "                elif key == \"Website\":\n",
    "                    key = \"Website\"\n",
    "                elif key == \"Address\":\n",
    "                    key = \"Address\"\n",
    "                data[key] = value\n",
    "\n",
    "    # Extracting principals, directors, and officers\n",
    "    principals_section = soup.find(\"div\", class_=\"overflowContainer\")\n",
    "    if principals_section:\n",
    "        table = principals_section.find(\"table\")\n",
    "        if table:\n",
    "            rows = table.find_all(\"tr\")\n",
    "            for i, row in enumerate(rows[1:], start=1):  # Skipping the header row\n",
    "                cols = row.find_all(\"td\")\n",
    "                if len(cols) >= 3:\n",
    "                    data[f\"Principal/Director/Officer {i} Name\"] = cols[0].get_text(strip=True)\n",
    "                    data[f\"Principal/Director/Officer {i} Role(s)\"] = cols[1].get_text(strip=True)\n",
    "                    data[f\"Principal/Director/Officer {i} Contact\"] = cols[2].get_text(strip=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Initialize an empty list to store all profile data\n",
    "all_profiles = []\n",
    "\n",
    "# Loop through each profile URL in the CSV file and scrape content\n",
    "for url in profile_links_df['Profile Link']:\n",
    "    profile_data = scrape_profile_content(url)\n",
    "    all_profiles.append(profile_data)\n",
    "    print(f\"Scraped data from {url}\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Save the scraped data to an Excel file\n",
    "df_profiles = pd.DataFrame(all_profiles)\n",
    "excel_file_path = \"Profile_Data.xlsx\"\n",
    "df_profiles.to_excel(excel_file_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {excel_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f436d3-7529-48f1-8356-656e11cfda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"Profile_Data.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Convert email column to lowercase\n",
    "df['Email'] = df['Email'].str.lower()\n",
    "\n",
    "# Define the conditions\n",
    "na_emails = df[df['Email'].isna()]\n",
    "regular_email_domains = r'@(gmail\\.com|yahoo\\.com|hotmail\\.com|aol\\.com|outlook\\.com|icloud\\.com|mail\\.com|zoho\\.com|protonmail\\.com|gmx\\.com|yandex\\.com|live\\.com|me\\.com|mac\\.com|msn\\.com)'\n",
    "regular_emails = df[df['Email'].str.contains(regular_email_domains, na=False, case=False)]\n",
    "company_emails = df[~df['Email'].str.contains(regular_email_domains, na=False, case=False) & df['Email'].notna()]\n",
    "\n",
    "# Split company emails into general and specific company emails\n",
    "general_company_domains = r'^(info@|general@|contact@|admin@|sales@|support@)'\n",
    "general_company_emails = company_emails[company_emails['Email'].str.contains(general_company_domains, na=False, case=False)]\n",
    "specific_company_emails = company_emails[~company_emails['Email'].str.contains(general_company_domains, na=False, case=False)]\n",
    "\n",
    "# Create a new Excel file with four sheets\n",
    "with pd.ExcelWriter(\"Profile_Data_Modified.xlsx\") as writer:\n",
    "    na_emails.to_excel(writer, sheet_name='NA Emails', index=False)\n",
    "    regular_emails.to_excel(writer, sheet_name='Regular Emails', index=False)\n",
    "    general_company_emails.to_excel(writer, sheet_name='General Company Emails', index=False)\n",
    "    specific_company_emails.to_excel(writer, sheet_name='Specific Company Emails', index=False)\n",
    "\n",
    "print(\"Data has been successfully split and saved into Profile_Data_Modified.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c534441-a5b3-4280-863b-421c05a9979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"Profile_Data_Modified.xlsx\"\n",
    "excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "# Create a new Excel writer object\n",
    "output_file_path = \"Profile_Data_Transformed.xlsx\"\n",
    "writer = pd.ExcelWriter(output_file_path, engine='xlsxwriter')\n",
    "\n",
    "# Function to pivot the data\n",
    "def pivot_data(df):\n",
    "    transformed_data = pd.DataFrame(columns=[\n",
    "        'URL', 'Vendor/Builder Legal Name', 'Doing Business As Name', 'Address', \n",
    "        'Website', 'Email', 'Phone Number', 'Fax Number', \n",
    "        'Principal/Director/Officer Name', 'Role(s)', 'Contact'\n",
    "    ])\n",
    "\n",
    "    rows_to_add = []\n",
    "    for index, row in df.iterrows():\n",
    "        has_principals = False\n",
    "        for i in range(1, 18):  # Adjust range to handle up to 7 entries\n",
    "            name_col = f'Principal/Director/Officer {i} Name'\n",
    "            role_col = f'Principal/Director/Officer {i} Role(s)'\n",
    "            contact_col = f'Principal/Director/Officer {i} Contact'\n",
    "            \n",
    "            if pd.notna(row[name_col]):\n",
    "                has_principals = True\n",
    "                new_row = {\n",
    "                    'URL': row['URL'],\n",
    "                    'Vendor/Builder Legal Name': row['Vendor/Builder Legal Name'],\n",
    "                    'Doing Business As Name': row['Doing Business As Name'],\n",
    "                    'Address': row['Address'],\n",
    "                    'Website': row['Website'],\n",
    "                    'Email': row['Email'],\n",
    "                    'Phone Number': row['Phone Number'],\n",
    "                    'Fax Number': row['Fax Number'],\n",
    "                    'Principal/Director/Officer Name': row[name_col],\n",
    "                    'Role(s)': row[role_col],\n",
    "                    'Contact': row[contact_col]\n",
    "                }\n",
    "                rows_to_add.append(new_row)\n",
    "        \n",
    "        # If no principals/officers were found, add a row with N/A\n",
    "        if not has_principals:\n",
    "            new_row = {\n",
    "                'URL': row['URL'],\n",
    "                'Vendor/Builder Legal Name': row['Vendor/Builder Legal Name'],\n",
    "                'Doing Business As Name': row['Doing Business As Name'],\n",
    "                'Address': row['Address'],\n",
    "                'Website': row['Website'],\n",
    "                'Email': row['Email'],\n",
    "                'Phone Number': row['Phone Number'],\n",
    "                'Fax Number': row['Fax Number'],\n",
    "                'Principal/Director/Officer Name': 'N/A',\n",
    "                'Role(s)': 'N/A',\n",
    "                'Contact': 'N/A'\n",
    "            }\n",
    "            rows_to_add.append(new_row)\n",
    "    \n",
    "    transformed_data = pd.concat([transformed_data, pd.DataFrame(rows_to_add)], ignore_index=True)\n",
    "    return transformed_data\n",
    "\n",
    "# Process each sheet in the Excel file\n",
    "for sheet_name in excel_file.sheet_names:\n",
    "    df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "    # Pivot the data\n",
    "    transformed_data = pivot_data(df)\n",
    "    # Write the transformed data to a new sheet in the output file\n",
    "    transformed_data.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Save the transformed data to the new Excel file\n",
    "writer.close()\n",
    "\n",
    "print(\"Data has been successfully transformed and saved into Profile_Data_Transformed.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d9dcea-aee5-40a8-8f17-d83d2cb758bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully transformed and saved into Profile_Data_Transformed.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"Profile_Data_Modified.xlsx\"\n",
    "excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "# Create a new Excel writer object\n",
    "output_file_path = \"Profile_Data_Transformed.xlsx\"\n",
    "writer = pd.ExcelWriter(output_file_path, engine='xlsxwriter')\n",
    "\n",
    "# Function to pivot the data\n",
    "def pivot_data(df):\n",
    "    transformed_data = pd.DataFrame(columns=[\n",
    "        'URL', 'Vendor/Builder Legal Name', 'Doing Business As Name', 'Address', \n",
    "        'Website', 'Email', 'Phone Number', 'Fax Number', \n",
    "        'Principal/Director/Officer Name', 'Role(s)', 'Contact'\n",
    "    ])\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        has_principals = False\n",
    "        for i in range(1, 27):  # Adjusted to 8 for up to 7 principals/directors/officers\n",
    "            name_col = f'Principal/Director/Officer {i} Name'\n",
    "            role_col = f'Principal/Director/Officer {i} Role(s)'\n",
    "            contact_col = f'Principal/Director/Officer {i} Contact'\n",
    "            \n",
    "            if pd.notna(row[name_col]):\n",
    "                has_principals = True\n",
    "                new_row = {\n",
    "                    'URL': row['URL'],\n",
    "                    'Vendor/Builder Legal Name': row['Vendor/Builder Legal Name'],\n",
    "                    'Doing Business As Name': row['Doing Business As Name'],\n",
    "                    'Address': row['Address'],\n",
    "                    'Website': row['Website'],\n",
    "                    'Email': row['Email'],\n",
    "                    'Phone Number': row['Phone Number'],\n",
    "                    'Fax Number': row['Fax Number'],\n",
    "                    'Principal/Director/Officer Name': row[name_col],\n",
    "                    'Role(s)': row[role_col],\n",
    "                    'Contact': row[contact_col]\n",
    "                }\n",
    "                transformed_data = pd.concat([transformed_data, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        \n",
    "        # If no principals/officers were found, add a row with N/A\n",
    "        if not has_principals:\n",
    "            new_row = {\n",
    "                'URL': row['URL'],\n",
    "                'Vendor/Builder Legal Name': row['Vendor/Builder Legal Name'],\n",
    "                'Doing Business As Name': row['Doing Business As Name'],\n",
    "                'Address': row['Address'],\n",
    "                'Website': row['Website'],\n",
    "                'Email': row['Email'],\n",
    "                'Phone Number': row['Phone Number'],\n",
    "                'Fax Number': row['Fax Number'],\n",
    "                'Principal/Director/Officer Name': 'N/A',\n",
    "                'Role(s)': 'N/A',\n",
    "                'Contact': 'N/A'\n",
    "            }\n",
    "            transformed_data = pd.concat([transformed_data, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "# Process each sheet in the Excel file\n",
    "for sheet_name in excel_file.sheet_names:\n",
    "    df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "    # Convert all values in the 'Email' column to strings and lowercase\n",
    "    df['Email'] = df['Email'].astype(str).str.lower()\n",
    "    # Sort the dataframe based on 'Vendor/Builder Legal Name'\n",
    "    df = df.sort_values(by='Vendor/Builder Legal Name')\n",
    "    # Pivot the data\n",
    "    transformed_data = pivot_data(df)\n",
    "    # Write the transformed data to a new sheet in the output file\n",
    "    transformed_data.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Save the transformed data to the new Excel file\n",
    "writer.close()\n",
    "\n",
    "print(\"Data has been successfully transformed and saved into Profile_Data_Transformed.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922c0388-44f7-4c63-9086-cdbe82aaa989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
