{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9fb62334-32ac-4649-90c1-b132d2749eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed page 1\n",
      "Processed page 2\n",
      "Processed page 3\n",
      "Processed page 4\n",
      "Processed page 5\n",
      "Processed page 6\n",
      "Processed page 7\n",
      "Processed page 8\n",
      "Processed page 9\n",
      "Processed page 10\n",
      "Processed page 11\n",
      "Processed page 12\n",
      "Processed page 13\n",
      "Processed page 14\n",
      "Processed page 15\n",
      "Processed page 16\n",
      "Processed page 17\n",
      "Processed page 18\n",
      "Processed page 19\n",
      "Processed page 20\n",
      "Processed page 21\n",
      "Processed page 22\n",
      "Processed page 23\n",
      "Processed page 24\n",
      "Processed page 25\n",
      "Processed page 26\n",
      "Processed page 27\n",
      "Processed page 28\n",
      "Processed page 29\n",
      "Processed page 30\n",
      "Processed page 31\n",
      "Processed page 32\n",
      "Processed page 33\n",
      "Processed page 34\n",
      "Processed page 35\n",
      "Processed page 36\n",
      "Processed page 37\n",
      "Processed page 38\n",
      "Processed page 39\n",
      "Processed page 40\n",
      "Processed page 41\n",
      "Processed page 42\n",
      "Processed page 43\n",
      "Processed page 44\n",
      "Processed page 45\n",
      "Processed page 46\n",
      "Processed page 47\n",
      "Processed page 48\n",
      "Processed page 49\n",
      "Processed page 50\n",
      "Processed page 51\n",
      "Processed page 52\n",
      "Processed page 53\n",
      "Processed page 54\n",
      "Processed page 55\n",
      "Processed page 56\n",
      "Processed page 57\n",
      "Processed page 58\n",
      "Processed page 59\n",
      "Processed page 60\n",
      "Processed page 61\n",
      "Processed page 62\n",
      "Processed page 63\n",
      "Processed page 64\n",
      "Processed page 65\n",
      "Processed page 66\n",
      "Company Names: 2100\n",
      "Addresses: 2100\n",
      "Phone Numbers: 2100\n",
      "Website URLs: 2100\n",
      "Company Titles: 2100\n",
      "Data saved to 'property_management_companies.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# web scrape from yellowpage\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "# Base URL for the Yellow Pages search\n",
    "base_url = 'https://www.yellowpages.ca/search/si'\n",
    "\n",
    "# Initialize lists to store the extracted data\n",
    "company_names = []\n",
    "addresses = []\n",
    "phone_numbers = []\n",
    "website_urls = []\n",
    "company_titles = []\n",
    "\n",
    "# Function to parse a single page\n",
    "def parse_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract company names\n",
    "    for listing in soup.find_all('div', class_='listing__content__wrap--flexed'):\n",
    "        name_tag = listing.find('a', class_='listing__name--link listing__link jsListingName')\n",
    "        address_tag = listing.find('span', class_='listing__address--full')\n",
    "        phone_tag = listing.find('a', class_='mlr__item__cta jsMlrMenu')\n",
    "        website_tag = listing.find('li', class_='mlr__item mlr__item--website')\n",
    "        title_tag = listing.find('div', class_='listing__headings')\n",
    "\n",
    "        # Extract and append company name\n",
    "        if name_tag:\n",
    "            company_names.append(name_tag.get_text(strip=True))\n",
    "        else:\n",
    "            company_names.append('N/A')\n",
    "\n",
    "        # Extract and append address\n",
    "        if address_tag:\n",
    "            addresses.append(address_tag.get_text(strip=True))\n",
    "        else:\n",
    "            addresses.append('N/A')\n",
    "\n",
    "        # Extract and append phone number\n",
    "        if phone_tag and 'data-phone' in phone_tag.attrs:\n",
    "            phone_numbers.append(phone_tag['data-phone'])\n",
    "        else:\n",
    "            phone_numbers.append('N/A')\n",
    "\n",
    "        # Extract and decode website URL\n",
    "        if website_tag:\n",
    "            a_tag = website_tag.find('a', class_='mlr__item__cta')\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                parsed_url = urllib.parse.urlparse(a_tag['href'])\n",
    "                query_params = urllib.parse.parse_qs(parsed_url.query)\n",
    "                redirect_url = query_params.get('redirect', [''])[0]\n",
    "                decoded_url = urllib.parse.unquote(redirect_url)\n",
    "                website_urls.append(decoded_url)\n",
    "            else:\n",
    "                website_urls.append('N/A')\n",
    "        else:\n",
    "            website_urls.append('N/A')\n",
    "\n",
    "        # Extract and append company title\n",
    "        if title_tag:\n",
    "            company_titles.append(title_tag.get_text(strip=True))\n",
    "        else:\n",
    "            company_titles.append('N/A')\n",
    "\n",
    "# Loop through multiple pages\n",
    "for page_num in range(1, 67):  # Assuming there are 66 pages\n",
    "    url = f'{base_url}/{page_num}/Property-Management/Toronto+ON'\n",
    "    try:\n",
    "        parse_page(url)\n",
    "        print(f\"Processed page {page_num}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process page {page_num}: {e}\")\n",
    "        break\n",
    "\n",
    "# Debugging output to check list lengths\n",
    "print(f\"Company Names: {len(company_names)}\")\n",
    "print(f\"Addresses: {len(addresses)}\")\n",
    "print(f\"Phone Numbers: {len(phone_numbers)}\")\n",
    "print(f\"Website URLs: {len(website_urls)}\")\n",
    "print(f\"Company Titles: {len(company_titles)}\")\n",
    "\n",
    "# Create a DataFrame to store the extracted data\n",
    "data = {\n",
    "    'Company Name': company_names,\n",
    "    'Address': addresses,\n",
    "    'Phone Number': phone_numbers,\n",
    "    'Website URL': website_urls,\n",
    "    'Company Title': company_titles\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "output_file = 'property_management_companies.xlsx'\n",
    "df.to_excel(output_file, index=False)\n",
    "print(f\"Data saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1c583c-2e0a-421b-95e5-64171292947b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tangn\\python data\n"
     ]
    }
   ],
   "source": [
    "#check file loc\n",
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac02138a-99b6-4b8e-9e3f-c7ef0a47703e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'property_management_companies.csv',\n",
       " 'property_management_companies.xlsx',\n",
       " 'property_management_companies_updated.xlsx',\n",
       " 'scraper yellow page.ipynb',\n",
       " 'yellowpages.html']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list file in loc\n",
    "os.listdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70ab1601-6d79-4fc0-b036-2b22ce159275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Company Name', 'Address', 'Phone Number', 'Website URL',\n",
      "       'Company Title'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#improt excel file and print coloumn\n",
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "file_path = 'property_management_companies.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Print the column names to verify\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f43b7e0f-c6e1-41da-90bc-8223b4eeb2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to property_management_companies_cleaned.xlsx\n"
     ]
    }
   ],
   "source": [
    "# find no duplicate rows\n",
    "# actually no longer needed\n",
    "df_cleaned = df.drop_duplicates(subset=['Company Name', 'Address', 'Phone Number', 'Website URL',\n",
    "       'Company Title'], keep='first')\n",
    "\n",
    "# Save the cleaned DataFrame to a new file\n",
    "cleaned_file_path = 'property_management_companies_cleaned.xlsx'\n",
    "df_cleaned.to_excel(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {cleaned_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df7f0f63-d16c-4b9a-8944-73d59ca88bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Company Name', 'Address', 'Phone Number', 'Website URL',\n",
      "       'Company Title'],\n",
      "      dtype='object')\n",
      "Data with unique website URLs saved to property_management_companies_unique_web.xlsx\n"
     ]
    }
   ],
   "source": [
    "# find unique website links\n",
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "file_path = 'property_management_companies.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Print the column names to verify\n",
    "print(df.columns)\n",
    "\n",
    "# Assuming the column is named 'Website URL' or 'Web'\n",
    "# Adjust the column name based on your actual data\n",
    "unique_web_df = df.drop_duplicates(subset=['Website URL'], keep='first')\n",
    "\n",
    "# Save the DataFrame with unique website URLs to a new file\n",
    "unique_web_file_path = 'property_management_companies_unique_web.xlsx'\n",
    "unique_web_df.to_excel(unique_web_file_path, index=False)\n",
    "\n",
    "print(f\"Data with unique website URLs saved to {unique_web_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09b85fb6-52f5-491e-994c-da54513ec8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data without 'facebook' URLs saved to property_management_companies_no_facebook.xlsx\n"
     ]
    }
   ],
   "source": [
    "# find unique websites and no more facebook pages anymore\n",
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "file_path = 'property_management_companies_unique_web.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Drop rows where the 'Website' column contains 'facebook'\n",
    "filtered_df = df[~df['Website URL'].str.contains('facebook', case=False, na=False)]\n",
    "\n",
    "# Save the filtered DataFrame to a new file\n",
    "filtered_file_path = 'property_management_companies_no_facebook.xlsx'\n",
    "filtered_df.to_excel(filtered_file_path, index=False)\n",
    "\n",
    "print(f\"Data without 'facebook' URLs saved to {filtered_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f4e537b-d8ef-4453-99e8-aeea5445df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'property_management_companies_domains.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# find domain and no www.\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Function to extract and clean the domain name from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "    # Remove 'www.' if present\n",
    "    if domain.startswith('www.'):\n",
    "        domain = domain[4:]\n",
    "    return domain if domain else url\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'property_management_companies_all_cleaned_407.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Clean and extract domain names\n",
    "df['Domain'] = df['Website URL'].apply(extract_domain)\n",
    "\n",
    "# Save the cleaned dataframe to a new file\n",
    "cleaned_file_path = 'property_management_companies_domains.xlsx'\n",
    "df.to_excel(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Data saved to '{cleaned_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee0280d9-7bfe-4168-86f9-cb68ee8586c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'property_management_companies_unique_domains.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# cleanup domains now unique domain names only\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Function to extract and clean the domain name from URL\n",
    "def extract_domain(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "    # Remove 'www.' if present\n",
    "    if domain.startswith('www.'):\n",
    "        domain = domain[4:]\n",
    "    return domain if domain else url\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'property_management_companies_domains.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Clean and extract domain names\n",
    "#df['Domain'] = df['Website URL'].apply(extract_domain)\n",
    "\n",
    "# Remove duplicate rows based on the Domain column\n",
    "df_unique = df.drop_duplicates(subset=['Domain'])\n",
    "\n",
    "# Save the cleaned dataframe to a new file\n",
    "cleaned_file_path = 'property_management_companies_unique_domains.xlsx'\n",
    "df_unique.to_excel(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Data saved to '{cleaned_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6153202-69e5-4cdd-9d27-7bdefbc1b408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
